"""
MSc Dissertation – Cybersecurity Incident Response & Recovery
German Financial Sector (2019–2024)

End-to-end pipeline:
1. Load data from GDELT Document API
2. Clean and preprocess text
3. Rule-based classification
4. ML models (Logistic Regression & SVM)
5. Output summary tables

Author: Rajesh
Programme: MSc Business Analytics
Year: 2025
"""

# =========================
# 1. Imports
# =========================
import requests
import pandas as pd
import re

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score

# =========================
# 2. Data Loading (ROBUST)
# =========================
def load_gdelt_data():
    url = "https://api.gdeltproject.org/api/v2/doc/doc"
    params = {
        "query": "Germany financial cyber attack",
        "mode": "ArtList",
        "format": "JSON",
        "maxrecords": 250,
        "sourcelang": "English"
    }

    response = requests.get(url, params=params, timeout=30)
    data = response.json()
    articles = data.get("articles", [])

    if not articles:
        return pd.DataFrame(columns=["text", "date", "source"])

    df_raw = pd.DataFrame(articles)

    df = pd.DataFrame()
    df["text"] = df_raw.get("title", "")
    df["date"] = df_raw.get("seendate", "")
    df["source"] = df_raw.get("domain", "unknown")

    return df

# =========================
# 3. Text Preprocessing
# =========================
STOP_WORDS = ENGLISH_STOP_WORDS

def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in STOP_WORDS]
    return " ".join(tokens)

# =========================
# 4. Rule-Based Classifier
# =========================
def rule_based_classifier(text):
    if any(word in text for word in ["restore", "resumed", "recovered"]):
        return "Recovery"
    elif any(word in text for word in ["investigation", "response", "mitigation"]):
        return "Response"
    else:
        return "Incident"

# =========================
# 5. Main Pipeline
# =========================
def main():
    print("Starting pipeline...")

    # Load data
    df = load_gdelt_data()
    print(f"Records loaded: {len(df)}")

    # Preprocess
    df["processed_text"] = df["text"].apply(preprocess_text)

    # Rule-based labels
    df["label"] = df["processed_text"].apply(rule_based_classifier)

    # Label distribution table
    label_table = df["label"].value_counts().reset_index()
    label_table.columns = ["Category", "Number_of_Events"]

    # =========================
    # 6. Machine Learning
    # =========================
    X = df["processed_text"]
    y = df["label"]

    vectorizer = TfidfVectorizer(max_features=5000)
    X_tfidf = vectorizer.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_tfidf, y, test_size=0.2, random_state=42
    )

    # Logistic Regression
    lr = LogisticRegression(max_iter=1000)
    lr.fit(X_train, y_train)
    lr_preds = lr.predict(X_test)

    # SVM
    svm = LinearSVC()
    svm.fit(X_train, y_train)
    svm_preds = svm.predict(X_test)

    # =========================
    # 7. Model Evaluation Table
    # =========================
    results = pd.DataFrame({
        "Model": ["Logistic Regression", "Support Vector Machine"],
        "Accuracy": [
            round(accuracy_score(y_test, lr_preds), 2),
            round(accuracy_score(y_test, svm_preds), 2)
        ],
        "F1_Score": [
            round(f1_score(y_test, lr_preds, average="weighted"), 2),
            round(f1_score(y_test, svm_preds, average="weighted"), 2)
        ]
    })

    # =========================
    # 8. Save Outputs
    # =========================
    label_table.to_csv("event_distribution.csv", index=False)
    results.to_csv("model_performance.csv", index=False)

    print("\nEvent distribution:")
    print(label_table)

    print("\nModel performance:")
    print(results)

    print("\nPipeline completed successfully.")

# =========================
# 9. Run
# =========================
if __name__ == "__main__":
    main()
